{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca0a11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eef09ee",
   "metadata": {},
   "source": [
    "## 1. 超参数 & 环境设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd116195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "max_input_len = 512\n",
    "max_target_len = 64\n",
    "num_epochs = 4\n",
    "learning_rate = 2e-5\n",
    "seed = 42\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8971f0",
   "metadata": {},
   "source": [
    "## 2. 数据加载 & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f62e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train dataset: 14520\n",
      "length of valid dataset: 984\n",
      "{'context': '第35集雪见缓缓张开眼睛，景天又惊又喜之际，长卿和紫萱的仙船驶至，见众人无恙，也十分高兴。众人登船，用尽合力把自身的真气和水分输给她。雪见终于醒过来了，但却一脸木然，全无反应。众人向常胤求助，却发现人世界竟没有雪见的身世纪录。长卿询问清微的身世，清微语带双关说一切上了天界便有答案。长卿驾驶仙船，众人决定立马动身，往天界而去。众人来到一荒山，长卿指出，魔界和天界相连。由魔界进入通过神魔之井，便可登天。众人至魔界入口，仿若一黑色的蝙蝠洞，但始终无法进入。后来花楹发现只要有翅膀便能飞入。于是景天等人打下许多乌鸦，模仿重楼的翅膀，制作数对翅膀状巨物。刚佩戴在身，便被吸入洞口。众人摔落在地，抬头发现魔界守卫。景天和众魔套交情，自称和魔尊重楼相熟，众魔不理，打了起来。', 'question': '仙剑奇侠传3第几集上天界', 'answer': '第35集'}\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(path):\n",
    "    examples = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            examples.append(json.loads(line.strip()))\n",
    "    return examples\n",
    "\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            \"context\":  item[\"context\"],\n",
    "            \"question\": item[\"question\"],\n",
    "            \"answer\":   item[\"answer\"],\n",
    "        }\n",
    "\n",
    "\n",
    "train_data = load_jsonl('./DuReaderQG/train.json')\n",
    "valid_data = load_jsonl('./DuReaderQG/dev.json')\n",
    "train_dataset = QADataset(train_data)\n",
    "valid_dataset = QADataset(valid_data)\n",
    "print(f\"length of train dataset: {len(train_dataset)}\")\n",
    "print(f\"length of valid dataset: {len(valid_dataset)}\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a919297f",
   "metadata": {},
   "source": [
    "## 3. Tokenizer 和 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7d7d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"../mengzi-t5-base\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(checkpoint, use_fast=False)\n",
    "model      = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8196bde",
   "metadata": {},
   "source": [
    "## 4. collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c19e661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch_sample):\n",
    "    inputs  = [f\"question: {example['question']}  context: {example['context']}\" for example in batch_sample]\n",
    "    targets = [example[\"answer\"] for example in batch_sample]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_len,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=max_target_len,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids\n",
    "\n",
    "    # 把[PAD]的id换成-100， 训练时忽略\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7a447",
   "metadata": {},
   "source": [
    "## 5. compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b189901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # 把 -100 还原成 pad_id，再 decode references\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_refs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    references = [decoded_refs]\n",
    "\n",
    "    bleu = sacrebleu.corpus_bleu(decoded_preds, references, force=True)\n",
    "    return {\n",
    "        \"bleu\":   np.mean(bleu.precisions) / 100,\n",
    "        \"bleu-1\": bleu.precisions[0] / 100,\n",
    "        \"bleu-2\": bleu.precisions[1] / 100,\n",
    "        \"bleu-3\": bleu.precisions[2] / 100,\n",
    "        \"bleu-4\": bleu.precisions[3] / 100,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e85bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = [\"2002年11月8日至14日\", \"比尔·拉塞尔\", \"南宁至东莞的距离\", \"其原创性及文中陈述内容未经本站证实,本地宝对本文及其中全部或者部分内容的真实性、完整性、及时性不作任何保证和承诺,请网友自行核实相关内容。\"]\n",
    "# preds_biased = [\"11月8日至14日\", \"比尔\", \"到东莞的距离\", \"其原创性及文中陈述内容未经本站证实,部分内容的真实性、完整性、及时性不作任何保证和承诺。\"]\n",
    "# source = [text]\n",
    "# input = tokenizer(text,\n",
    "#                   padding=True,\n",
    "#                   truncation=True,\n",
    "#                   max_length = max_input_len,\n",
    "#                   return_tensors=\"pt\")[\"input_ids\"]\n",
    "# preds = tokenizer.batch_decode(input, skip_special_tokens=True)\n",
    "# bleu = sacrebleu.corpus_bleu(preds_biased, source, force=True)\n",
    "# print(preds_biased)\n",
    "# print(source)\n",
    "# print(np.mean(bleu.precisions))\n",
    "# print(bleu.precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78903e5e",
   "metadata": {},
   "source": [
    "## 6. 配置trainingarguments & trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77343fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./outputs\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "    predict_with_generate=True,          # 让 Trainer 调用 model.generate\n",
    "    generation_num_beams=5,\n",
    "    generation_max_length=max_target_len,\n",
    "\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c13a395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n",
      "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:474: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e02cf",
   "metadata": {},
   "source": [
    "## 7. 训练 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd9b8ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14520\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7260\n",
      "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7260' max='7260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7260/7260 18:35, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Bleu-1</th>\n",
       "      <th>Bleu-2</th>\n",
       "      <th>Bleu-3</th>\n",
       "      <th>Bleu-4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.663300</td>\n",
       "      <td>0.941027</td>\n",
       "      <td>0.537765</td>\n",
       "      <td>0.577653</td>\n",
       "      <td>0.603645</td>\n",
       "      <td>0.501873</td>\n",
       "      <td>0.467890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.334400</td>\n",
       "      <td>0.950523</td>\n",
       "      <td>0.514600</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.564482</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>0.440299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.236600</td>\n",
       "      <td>1.016177</td>\n",
       "      <td>0.622517</td>\n",
       "      <td>0.600438</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>1.071698</td>\n",
       "      <td>0.592385</td>\n",
       "      <td>0.604451</td>\n",
       "      <td>0.630807</td>\n",
       "      <td>0.547325</td>\n",
       "      <td>0.586957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 984\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./outputs/checkpoint-1815\n",
      "Configuration saved in ./outputs/checkpoint-1815/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1815/pytorch_model.bin\n",
      "tokenizer config file saved in ./outputs/checkpoint-1815/tokenizer_config.json\n",
      "Special tokens file saved in ./outputs/checkpoint-1815/special_tokens_map.json\n",
      "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 984\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./outputs/checkpoint-3630\n",
      "Configuration saved in ./outputs/checkpoint-3630/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3630/pytorch_model.bin\n",
      "tokenizer config file saved in ./outputs/checkpoint-3630/tokenizer_config.json\n",
      "Special tokens file saved in ./outputs/checkpoint-3630/special_tokens_map.json\n",
      "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 984\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./outputs/checkpoint-5445\n",
      "Configuration saved in ./outputs/checkpoint-5445/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5445/pytorch_model.bin\n",
      "tokenizer config file saved in ./outputs/checkpoint-5445/tokenizer_config.json\n",
      "Special tokens file saved in ./outputs/checkpoint-5445/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-1815] due to args.save_total_limit\n",
      "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 984\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./outputs/checkpoint-7260\n",
      "Configuration saved in ./outputs/checkpoint-7260/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7260/pytorch_model.bin\n",
      "tokenizer config file saved in ./outputs/checkpoint-7260/tokenizer_config.json\n",
      "Special tokens file saved in ./outputs/checkpoint-7260/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-3630] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./outputs/checkpoint-5445 (score: 0.6225166758105947).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 984\n",
      "  Batch size = 8\n",
      "/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [123/123 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FINAL EVAL] {'eval_loss': 1.0161772966384888, 'eval_bleu': 0.6225166758105947, 'eval_bleu-1': 0.6004382761139518, 'eval_bleu-2': 0.6493506493506493, 'eval_bleu-3': 0.5777777777777778, 'eval_bleu-4': 0.6625, 'eval_runtime': 39.2246, 'eval_samples_per_second': 25.086, 'eval_steps_per_second': 3.136, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "print(\"[FINAL EVAL]\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2829361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./final_model\n",
      "Configuration saved in ./final_model/config.json\n",
      "Model weights saved in ./final_model/pytorch_model.bin\n",
      "tokenizer config file saved in ./final_model/tokenizer_config.json\n",
      "Special tokens file saved in ./final_model/special_tokens_map.json\n",
      "tokenizer config file saved in ./final_model/tokenizer_config.json\n",
      "Special tokens file saved in ./final_model/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./final_model/tokenizer_config.json',\n",
       " './final_model/special_tokens_map.json',\n",
       " './final_model/spiece.model',\n",
       " './final_model/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab0fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
